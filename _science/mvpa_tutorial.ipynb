{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVPA Tutorial\n",
    "\n",
    "In this tutorial, you will be using python and a few packages to perform MVPA analysis. This is a difficult type of analysis that takes a long time to master, so we'll start out with showing you some basic operations. \n",
    "Please don't let the programming code scare you, but try to focus on the figures that are produced by the analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python is a lean programming language, \n",
    "# which means that we have to explicitly import specialized functionality\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import nilearn, sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does fitting work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the problems of underfitting and overfitting. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the (noisy) samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees, the more degrees, the more parameters. If your model is too simple, it will not describe the data well. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import cross_validation\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 50\n",
    "degrees = [1, 2, 4, 8]\n",
    "\n",
    "true_fun = lambda X: np.cos(1.5 * np.pi * X)\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax = plt.subplot(111)\n",
    "plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=1,\n",
    "                                         include_bias=False)\n",
    "linear_regression = LinearRegression()\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"linear_regression\", linear_regression)])\n",
    "pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "# Evaluate the models using crossvalidation\n",
    "scores = cross_validation.cross_val_score(pipeline,\n",
    "    X[:, np.newaxis], y, scoring=\"mean_squared_error\", cv=10)\n",
    "\n",
    "X_test = np.linspace(0, 1, 100)\n",
    "plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model, degree 1: Too Simple\")\n",
    "plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "plt.scatter(X, y, label=\"Samples\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "    degrees[0], -scores.mean(), scores.std()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making our model more complex.\n",
    "If we have more data points, we can add complexity to our model, but not too much! Here, we're using different model complexities, which illustrate how important it is that you don't fit a model that is overly complex (overfitting), or overly simple (underfitting). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_validation.cross_val_score(pipeline,\n",
    "        X[:, np.newaxis], y, scoring=\"mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see, for high degrees the model will overfit the training data, i.e. it learns the noise that is specific to the training data. Imagine you were going to use an independent dataset to test whether the classification algorithm has done a good job. Which would you choose?\n",
    "\n",
    "*Further explanation:*\n",
    "\n",
    "Our cross-validation shows that a polynomial of degree 4 approximates the true function almost perfectly.  Technically, in this example we evaluate quantitatively whether we are overfitting / underfitting by using cross-validation.  As a measure of fit quality, we calculate the mean squared error (MSE) on a validation set, the higher, the less likely the model generalizes correctly from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between different classifiers\n",
    "\n",
    "In this first example, we're going to explore some differences between different classifiers. \n",
    "One big difference between classifiers is whether they are 'linear' or not. You can think of linear in this context as being constrained to very simple straight (*linear*) discriminating lines. \n",
    "\n",
    "Machine Learning is a very cool hot field in data analysis, being used by the likes of Google and Facebook. If you are interested in the mechanisms behind the different methods, I recomment the Coursera course Machine Learning, from Stanford https://www.coursera.org/learn/machine-learning. For now, we won't dive into the specifics of these methods lest we get too mathematical. Take home message here is that depending on the characteristics of a given problem, one will normally choose a method that suits these characteristics to optimize the performance of the classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all the specific classifiers that we'll use in the first example.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the classifiers and some data to classify. \n",
    "Since we generate the data ourselves, we can play with the amount of noise in the classification problem, making it easier and/or harder to perform. The amount of noise is set in the last lines of the cell below. Once you have run this section up until the plots are produced, you can try running this section again with a higher amount of noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create classifiers and data. \n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear Support Vector Machine\\n(SVM)\", \"Radial Basis Function SVM\", \"Decision Tree\",\n",
    "         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"Linear Discriminant Analysis\",\n",
    "         \"Quadratic Discriminant Analysis\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [linearly_separable,\n",
    "            make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we describe our data by plotting them. Looking at your data is always extremely important, it's the only way to 'see' what's really going on! Training points are plotted as circles, test points are plotted as triangles. The two classes are shown in different colors, red and blue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 5))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds in datasets:\n",
    "    ax = plt.subplot(1, len(datasets), i)\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, s = 60)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.3, s = 60, marker = 'v')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title('Dataset %i'%i)\n",
    "    i += 1\n",
    "\n",
    "figure.subplots_adjust(left=.02, right=.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the patterns in the data? Can you think of a simple rule (like drawing a line between red and blue) that would distinguish between the two colors, or classes? Which classification problem do you think is easiest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds in datasets:\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "figure.subplots_adjust(left=.02, right=.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundaries for each of the classification algorithms are shown in the underlying colors, from red to blue. Some algorithms, by virtue of their architecture, will only draw single straight decision boundaries, others can draw multiple straight decision boundaries, and yet others can draw curved decision boundaries. \n",
    "\n",
    "**Try to answer the following questions:**\n",
    "1. How do the linear versions of *Discriminant analysis* and *Support Vector Machines (SVM)* go wrong? \n",
    "2. What is the difference between classification problem 1 and 2&3, which makes it more amenable to classification by the linear SVM and Discriminant analysis? \n",
    "3. Try to think of a downside of using a classifier that can draw very complex decision boudaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NILearn: neuroimaging analysis in a notebook  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NILearn [http://nilearn.github.io] is a package that allows us to visualize and analyze neuroimaging data. It performs Machine Learning type analyses from sklearn (which we just used the classifiers from) on brain imaging data easily. \n",
    "\n",
    "First, an example on how we would plot neuroimaging data using this package. We will plot the masks we will use for a following analysis. What this means is that we'll take data from these specific regions to train our MVPA algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "\n",
    "haxby_dataset = datasets.fetch_haxby(n_subjects=1)\n",
    "func_filename = haxby_dataset.func[0]\n",
    "\n",
    "plotting.plot_glass_brain(haxby_dataset['mask_face'][0], title = 'Face-selective Regions');\n",
    "plotting.plot_glass_brain(haxby_dataset['mask_house'][0], title = 'House-selective Regions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, an example of statistics of a simple task plotted in a glass brain. You can play around with the threshold values in the last lines of the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localizer_dataset = datasets.fetch_localizer_contrasts(\n",
    "    contrasts = ['auditory&visual motor vs cognitive processing', 'checkerboard'], \n",
    "    n_subjects = 1, get_tmaps = True)\n",
    "\n",
    "plotting.plot_glass_brain(localizer_dataset.cmaps[1], threshold=1.5, colorbar=True,\n",
    "                          plot_abs=False, title = 'Perception vs Cognition', display_mode='ortho');\n",
    "plotting.plot_glass_brain(localizer_dataset.tmaps[1], threshold=1, colorbar=True,\n",
    "                          plot_abs=False, title = 'High-contrast visual stimulus', display_mode='ortho');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI-based decoding analysis in Haxby et al. dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we reproduce the data analysis conducted by Haxby et al. in *“Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex”*, the first Science paper that started the field of MVPA.\n",
    "\n",
    "#### The Haxby 2001 experiment\n",
    "Subjects are presented visual stimuli from different categories. We are going to predict which category the subject is seeing from the fMRI activity recorded in masks of the ventral stream. Significant prediction shows that the signal in the region contains information on the corresponding category.\n",
    "\n",
    "Specifically, we look at decoding accuracy for different objects in three different masks: the full ventral stream (**mask_vt**), the house selective areas (**mask_house**) and the face selective areas (**mask_face**), that have been defined via a standard GLM-based analysis.\n",
    "\n",
    "First we load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nilearn NiftiMasker, the practical masking and unmasking tool\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "# load labels\n",
    "import numpy as np\n",
    "labels = np.recfromcsv(haxby_dataset.session_target[0], delimiter=\" \")\n",
    "stimuli = labels['labels']\n",
    "\n",
    "# identify resting state labels in order to be able to remove them\n",
    "resting_state = stimuli == b\"rest\"\n",
    "\n",
    "# find names of remaining active labels\n",
    "categories = np.unique(stimuli[np.logical_not(resting_state)])\n",
    "\n",
    "# extract tags indicating to which acquisition run a tag belongs\n",
    "session_labels = labels[\"chunks\"][np.logical_not(resting_state)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use scikit-learn for decoding on the different masks. With all the different stimulus categories this may take a while. The program will keep you posted on its results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classifier: a support vector classifier\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(C=1., kernel=\"linear\")\n",
    "\n",
    "# A classifier to set the chance level\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_classifier = DummyClassifier()\n",
    "\n",
    "# Make a data splitting object for cross validation\n",
    "from sklearn.cross_validation import LeaveOneLabelOut, cross_val_score\n",
    "cv = LeaveOneLabelOut(session_labels)\n",
    "\n",
    "mask_names = ['mask_vt', 'mask_face', 'mask_house']\n",
    "\n",
    "mask_scores = {}\n",
    "mask_chance_scores = {}\n",
    "\n",
    "for mask_name in mask_names:\n",
    "    print(\"Working on mask %s\" % mask_name)\n",
    "    # For decoding, standardizing is often very important\n",
    "    mask_filename = haxby_dataset[mask_name][0]\n",
    "    masker = NiftiMasker(mask_img=mask_filename, standardize=True)\n",
    "    masked_timecourses = masker.fit_transform(\n",
    "        func_filename)[np.logical_not(resting_state)]\n",
    "\n",
    "    mask_scores[mask_name] = {}\n",
    "    mask_chance_scores[mask_name] = {}\n",
    "\n",
    "    for category in categories:\n",
    "        print(\"Processing %s %s\" % (mask_name, category))\n",
    "        task_mask = np.logical_not(resting_state)\n",
    "        classification_target = (stimuli[task_mask] == category)\n",
    "        mask_scores[mask_name][category] = cross_val_score(\n",
    "            classifier,\n",
    "            masked_timecourses,\n",
    "            classification_target,\n",
    "            cv=cv, scoring=\"f1\")\n",
    "\n",
    "        mask_chance_scores[mask_name][category] = cross_val_score(\n",
    "            dummy_classifier,\n",
    "            masked_timecourses,\n",
    "            classification_target,\n",
    "            cv=cv, scoring=\"f1\")\n",
    "\n",
    "        print(\"Scores: %1.2f +- %1.2f\" % (\n",
    "            mask_scores[mask_name][category].mean(),\n",
    "            mask_scores[mask_name][category].std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a simple bar plot to summarize the results. Can you interpret these results?\n",
    "\n",
    "For example, where (in what mask) is the classification accuracy for houses best? And in what region can you best decode faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "tick_position = np.arange(len(categories))\n",
    "plt.xticks(tick_position, categories, rotation=45)\n",
    "\n",
    "for color, mask_name in zip('rgb', mask_names):\n",
    "    score_means = [mask_scores[mask_name][category].mean()\n",
    "                   for category in categories]\n",
    "    plt.bar(tick_position, score_means, label=mask_name,\n",
    "            width=.25, color=color)\n",
    "\n",
    "    score_chance = [mask_chance_scores[mask_name][category].mean()\n",
    "                    for category in categories]\n",
    "    plt.bar(tick_position, score_chance,\n",
    "            width=.25, edgecolor='k', facecolor='none')\n",
    "\n",
    "    tick_position = tick_position + .2\n",
    "\n",
    "plt.ylabel('Classification accurancy (f1 score)')\n",
    "plt.xlabel('Visual stimuli category')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Category-specific classification accuracy for different masks')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searchlight decoding analysis\n",
    "\n",
    "Searchlight was introduced in Information-based functional brain mapping, Nikolaus Kriegeskorte, Rainer Goebel and Peter Bandettini (PNAS 2006) and consists of scanning the images volume with a searchlight. Briefly, a ball of given radius is scanned across the brain volume and the prediction accuracy of a classifier trained on the corresponding voxels is measured.\n",
    "\n",
    "For simplicity, we are going to decode from exactly the same experiment as the ROI-based analysis above. So, it's just houses, faces and objects that we're trying to distinguish from one another. \n",
    "\n",
    "First we download some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import new_img_like\n",
    "from nilearn.image import index_img\n",
    "from nibabel import load\n",
    "\n",
    "haxby_dataset = datasets.fetch_haxby_simple()\n",
    "\n",
    "fmri_filename = haxby_dataset.func[0]\n",
    "y, session = np.loadtxt(haxby_dataset.session_target[0]).astype('int').T\n",
    "conditions = np.recfromtxt(haxby_dataset.conditions_target[0])['f0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set things up and fit the Searchlight decoder on a single slice of data (otherwise it would take too much time). Keep in mind that this may take up to a minute to process. If you see 'error' messages popping up, don't worry too much. Just try the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_mask = np.logical_or(conditions == b'face', conditions == b'house')\n",
    "\n",
    "fmri_img = index_img(fmri_filename, condition_mask)\n",
    "y, session = y[condition_mask], session[condition_mask]\n",
    "conditions = conditions[condition_mask]\n",
    "\n",
    "mask_img = load(haxby_dataset.mask)\n",
    "\n",
    "# .astype() makes a copy.\n",
    "process_mask = mask_img.get_data().astype(np.int)\n",
    "picked_slice = 27\n",
    "process_mask[..., (picked_slice + 1):] = 0\n",
    "process_mask[..., :picked_slice] = 0\n",
    "process_mask[:, 30:] = 0\n",
    "process_mask_img = new_img_like(mask_img, process_mask)\n",
    "\n",
    "# Make processing parallel\n",
    "# /!\\ As each thread will print its progress, n_jobs > 1 could mess up the\n",
    "#     information output.\n",
    "n_jobs = 1\n",
    "\n",
    "# Define the cross-validation scheme used for validation.\n",
    "# Here we use a KFold cross-validation on the session, which corresponds to\n",
    "# splitting the samples in 4 folds and make 4 runs using each fold as a test\n",
    "# set once and the others as learning sets\n",
    "from sklearn.cross_validation import KFold\n",
    "cv = KFold(y.size, n_folds=4)\n",
    "\n",
    "import nilearn.decoding\n",
    "# The radius is the one of the Searchlight sphere that will scan the volume\n",
    "searchlight = nilearn.decoding.SearchLight(\n",
    "    mask_img,\n",
    "    process_mask_img=process_mask_img,\n",
    "    radius=5.6, n_jobs=n_jobs,\n",
    "    verbose=1, cv=cv)\n",
    "searchlight.fit(fmri_img, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to prepare the results for plotting, and perform the actual plotting. This time, we're not using a glass brain, but the background image is our actual functional data. Looks less nice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "# For decoding, standardizing is often very important\n",
    "nifti_masker = NiftiMasker(mask_img=mask_img, sessions=session,\n",
    "                           standardize=True, memory='nilearn_cache',\n",
    "                           memory_level=1)\n",
    "fmri_masked = nifti_masker.fit_transform(fmri_img)\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "f_values, p_values = f_classif(fmri_masked, y)\n",
    "p_values = -np.log10(p_values)\n",
    "p_values[p_values > 10] = 10\n",
    "p_unmasked = nifti_masker.inverse_transform(p_values).get_data()\n",
    "\n",
    "# Use the fmri mean image as a surrogate of anatomical data\n",
    "from nilearn import image\n",
    "mean_fmri = image.mean_img(fmri_img)\n",
    "\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "plotting.plot_stat_map(new_img_like(mean_fmri, searchlight.scores_), mean_fmri,\n",
    "              title=\"Searchlight\", cut_coords = [-4, -65, -16],\n",
    "              colorbar=False)\n",
    "\n",
    "# F_score results\n",
    "p_ma = np.ma.array(p_unmasked, mask=np.logical_not(process_mask))\n",
    "plotting.plot_stat_map(new_img_like(mean_fmri, p_ma), mean_fmri,\n",
    "              title=\"F-scores\", cut_coords = [-4, -65, -16],\n",
    "              colorbar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
